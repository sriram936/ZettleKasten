
### **1.Â _Differential Equations and Linear Algebra_Â (2018)**

**Focus:**

- **Mathematical foundations**Â (linear algebra + differential equations).
    
- **Theoretical depth**Â with applications in engineering, physics, and dynamical systems.
    

**Best for:**

- People interested inÂ **mathematical modeling, dynamical systems, or scientific computing**.
    
- Those who want aÂ **strong theoretical basis**Â in linear algebraÂ **combined with differential equations**Â (useful for control theory, optimization, and PDEs in ML).
    

**Relevance to Core ML:**  
âœ”Â **Linear algebra**Â (SVD, eigenvectors, matrix decompositions) isÂ **essential for ML**.  
âœ” Differential equations areÂ **less directly relevant**Â to ML (unless working onÂ **neural ODEs, reinforcement learning, or physics-informed ML**).

---

### **2.Â _Linear Algebra and Learning from Data_Â (2019)**

**Focus:**

- **Applied linear algebra**Â (with direct applications to ML and data science).
    
- **Learning from data**Â (regression, PCA, deep learning, optimization).
    

**Best for:**

- **Core ML researchers & practitioners**Â who wantÂ **linear algebra + ML applications**.
    
- People who preferÂ **intuitive explanations**Â (Strangâ€™s style) withÂ **real-world ML examples**.
    

**Relevance to Core ML:**  
âœ”Â **Covers PCA, least squares, SGD, and neural networks**â€”directly applicable to ML.  
âœ” ExplainsÂ **how linear algebra powers ML algorithms**Â (e.g., backpropagation, embeddings).  
âœ” MoreÂ **modern and ML-focused**Â than the 2018 book.

---

### **Which One Should You Choose?**

|**Criteria**|**Differential Eqns & LA (2018)**|**LA & Learning from Data (2019)**|
|---|---|---|
|**Primary Focus**|Math theory + DEs|Applied LA + ML|
|**Best for ML Core?**|âŒ (Unless doing NN theory)|âœ… (Directly relevant)|
|**Covers Deep Learning?**|âŒ|âœ… (Neural nets, backprop)|
|**Useful for Optimization?**|âœ… (ODE/PDE connections)|âœ… (SGD, least squares)|
|**Readability**|Dense (math-heavy)|More intuitive, ML examples|

---

### **Final Recommendation**

- **For core ML people**,Â **"Linear Algebra and Learning from Data" (2019)**Â is theÂ **clear winner**â€”itâ€™sÂ **more aligned with modern ML**Â (PCA, deep learning, optimization).
    
- TheÂ **2018 book**Â is better forÂ **mathematicians/physicists**Â needingÂ **differential equations**Â alongside linear algebra.
    

**If you already know linear algebra well**, the 2019 book will help youÂ **connect it to ML**. If you needÂ **pure math foundations first**, the 2018 book is better.

Would you like aÂ **chapter-wise breakdown**Â of the 2019 bookâ€™s ML-relevant content? ğŸš€

<hr>
<hr>



### **Comparison: Boyd & Vandenbergheâ€™sÂ _Convex Optimization_Â vs.Â _Applied Linear Algebra_**

Both books areÂ **excellent**Â and written by leading experts, but they serveÂ **different purposes**Â in machine learning (ML) and optimization. Hereâ€™s a breakdown to help you choose:

---

### **1.Â _Convex Optimization_Â (2004, Updated 2018)**

**Focus:**

- **Theory and algorithms for convex optimization**Â (gradient descent, duality, SOCP, SDP).
    
- **Mathematical rigor**Â with proofs, but also practical applications.
    

**Best for:**

- **ML researchers**Â working onÂ **optimization theory**Â (e.g., SVM, neural net training, reinforcement learning).
    
- **Those who need deep intuition**Â on gradient methods, Lagrange multipliers, and convexity.
    
- **People implementing custom optimizers**Â (e.g., for constrained ML problems).
    

**Relevance to Core ML:**  
âœ”Â **Key for understanding optimization in ML**Â (SGD, ADAM, proximal methods).  
âœ” CoversÂ **duality**Â (important for SVMs, kernel methods).  
âœ” Used inÂ **advanced ML courses**Â (e.g., Stanfordâ€™s EE364).

**Limitations:**  
âŒÂ **Assumes strong math background**Â (real analysis, linear algebra).  
âŒÂ **Not focused on ML directly**â€”more general optimization theory.

---

### **2.Â _Introduction to Applied Linear Algebra_Â (2018)**

**Focus:**

- **Practical linear algebra**Â (vectors, matrices, least squares, PCA).
    
- **Applications in data science, ML, and signal processing**.
    

**Best for:**

- **Beginners in ML/data science**Â who needÂ **intuitive LA explanations**.
    
- **Engineers/applied scientists**Â who useÂ **least squares, regression, and PCA**.
    
- **People who prefer computation over proofs**Â (uses Python examples).
    

**Relevance to Core ML:**  
âœ”Â **Least squares = foundation of linear regression, ridge regression, and deep learning initialization**.  
âœ”Â **PCA, SVD, and matrix factorizations**Â (essential for embeddings, recommender systems).  
âœ”Â **More accessible**Â thanÂ _Convex Optimization_Â (fewer proofs, more applications).

**Limitations:**  
âŒÂ **Doesnâ€™t cover deep learning or advanced optimization**.  
âŒÂ **Less theoretical**Â (wonâ€™t help with convergence proofs).

---

### **Which One Should You Choose?**

|**Criteria**|_Convex Optimization_|_Applied Linear Algebra_|
|---|---|---|
|**Math Level**|Advanced (grad-level)|Beginner-friendly|
|**Focus**|Optimization theory|Practical LA + regression|
|**Best for ML Core?**|âœ… (Optimization theory)|âœ… (Foundations of ML)|
|**Covers Deep Learning?**|Indirectly (SGD, etc.)|âŒ (Only basic LA)|
|**Useful for Implementers?**|âœ… (Custom optimizers)|âœ… (Regression, PCA)|
|**Python Examples?**|âŒ (Mostly math)|âœ… (NumPy included)|

---

### **Final Recommendation**

- **For ML researchers & optimization theory**:Â **"Convex Optimization"**Â is aÂ **must-read**.
    
- **For applied ML engineers & beginners**:Â **"Applied Linear Algebra"**Â isÂ **more practical and accessible**.
    

**If you can, read both**Â (in order):

1. Start withÂ _Applied Linear Algebra_Â to build intuition.
    
2. Move toÂ _Convex Optimization_Â for deeper theory.