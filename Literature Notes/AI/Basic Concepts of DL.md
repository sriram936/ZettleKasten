Author: Misra Turp 
Source: YT
1) Introduction

2) Forward Propagation
	![](/ZettleKasten/Unsorted/Attachment/Pasted_image_20250710234804.png)
	where x1, x2 are inputs
	w1, w2 are bases
	b = bias
	alpha = activation function sigmoid, tanh(x), RELU
	
	A = alpha(WT * X + b)
	![](/ZettleKasten/Unsorted/Attachment/Pasted_image_20250711000124.png)

![](/ZettleKasten/Unsorted/Attachment/Pasted_image_20250711000210.png)![](/ZettleKasten/Unsorted/Attachment/Pasted_image_20250711000406.png)
![](/ZettleKasten/Unsorted/Attachment/Pasted_image_20250711000549.png)

3) Back Propagation